# Guia de Migra√ß√£o - Vers√£o 1.0 para 2.0

**Data**: 09 de Dezembro de 2025  
**Vers√£o**: 2.0.0

---

## Vis√£o Geral

Este guia auxilia na migra√ß√£o do c√≥digo legado (`DataBricks.py`) para a nova arquitetura modular (v2.0).

---

## Backup

O arquivo original foi preservado como `src/DataBricks.py.backup` e pode ser restaurado se necess√°rio.

---

## Mudan√ßas de Estrutura

### Antes (v1.0)
```
src/
‚îî‚îÄ‚îÄ DataBricks.py  (479 linhas - tudo em um arquivo)
```

### Depois (v2.0)
```
src/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ config.py
‚îú‚îÄ‚îÄ logger.py
‚îú‚îÄ‚îÄ scraper.py
‚îú‚îÄ‚îÄ ai_processor.py
‚îú‚îÄ‚îÄ n8n_integration.py
‚îú‚îÄ‚îÄ csv_handler.py
‚îú‚îÄ‚îÄ database.py
‚îî‚îÄ‚îÄ utils.py
```

---

## Mudan√ßas de Configura√ß√£o

### API Keys

**Antes**: Carregada diretamente
```python
load_dotenv()
client = openai.Client()
```

**Depois**: Centralizada em config
```python
# No arquivo .env
OPENAI_API_KEY=sua_chave_aqui

# No c√≥digo
from src.config import config
# config.openai_api_key j√° est√° dispon√≠vel
```

### URLs e Paths

**Antes**: Hardcoded no c√≥digo
```python
BASE = "https://www.databricks.com"
CATEGORY_URL = "https://www.databricks.com/blog/category/platform"
OUTPUT_CSV = "databricks_platform_posts.csv"
```

**Depois**: Externalizadas em config.ini
```ini
[scraper]
base_url = https://www.databricks.com
category_url = https://www.databricks.com/blog/category/platform

[files]
output_posts_csv = databricks_platform_posts.csv
```

---

## Mudan√ßas de C√≥digo

### 1. Scraping

**Antes**:
```python
driver = webdriver.Chrome(service=service, options=options)
driver.get(CATEGORY_URL)
# ... c√≥digo inline ...
driver.quit()
```

**Depois**:
```python
from src.scraper import DatabricksScraper

scraper = DatabricksScraper()
posts = scraper.scrape_posts()
scraper.cleanup()
```

### 2. Processamento com IA

**Antes**:
```python
def gerar_resumo_do_link(link):
    mensagens = [...]
    resposta = client.chat.completions.create(...)
    return resposta.choices[0].message.content

# Loop manual pelos posts
for index, row in df.iterrows():
    resumo = gerar_resumo_do_link(link)
    # ... salvar ...
```

**Depois**:
```python
from src.ai_processor import AIPostProcessor

processor = AIPostProcessor()
processed_posts = processor.process_posts(posts)
```

### 3. Integra√ß√£o n8n

**Antes**:
```python
def enviar_para_n8n(posts):
    response = requests.post(WEBHOOK_URL, json=posts)
    print("Status:", response.status_code)

posts = carregar_posts(csv_path)
enviar_para_n8n(posts)
```

**Depois**:
```python
from src.n8n_integration import N8NIntegration

integration = N8NIntegration()
success = integration.send_posts(posts)
```

### 4. Logging

**Antes**:
```python
print("üìÑ Lendo CSV de posts...")
print(f"üîé {len(df)} posts encontrados.\n")
```

**Depois**:
```python
from src.logger import get_logger

logger = get_logger(__name__)
logger.info("Lendo CSV de posts")
logger.info(f"Encontrados {len(df)} posts")
```

### 5. CSV Operations

**Antes**:
```python
df = pd.read_csv(INPUT_CSV)
# ... manipula√ß√µes ...
df.to_csv(OUTPUT_CSV, index=False)
```

**Depois**:
```python
from src.csv_handler import CSVHandler

csv_handler = CSVHandler()
posts = csv_handler.load_posts()
# ... manipula√ß√µes ...
csv_handler.save_posts(posts)
```

---

## Execu√ß√£o

### Antes (v1.0)

```bash
python src/DataBricks.py
```

O script executava tudo sequencialmente:
1. Scraping
2. Processamento
3. Integra√ß√£o n8n

### Depois (v2.0)

```bash
# Pipeline completo
python src/main.py

# Ou import program√°tico
from src.main import Application

app = Application()
app.run_full_pipeline()

# Ou fases individuais
app.run_scraping()
app.run_ai_processing()
app.run_n8n_integration()
```

---

## Compatibilidade

### Dados

‚úÖ **Compat√≠vel**: Todos os dados existentes continuam funcionando

- CSV: Mesma estrutura, colunas adicionais criadas automaticamente
- JSON: Formato preservado
- Database: Novo (n√£o afeta dados existentes)

### Configura√ß√£o

‚ö†Ô∏è **Requer Setup**: Novos arquivos de configura√ß√£o

1. Criar `config.ini` (fornecido)
2. Criar `.env` com `OPENAI_API_KEY`
3. Ajustar configura√ß√µes conforme necess√°rio

### Comportamento

‚úÖ **Preservado**: Funcionalidade mantida

- Scraping: Mesmos resultados
- Processamento: Mesma l√≥gica de resumos
- Integra√ß√£o: Mesmo formato de envio

### Melhorias Adicionais

üéâ **Novos Recursos**:

- Logs estruturados em arquivo
- Banco de dados para tracking
- Estat√≠sticas detalhadas
- Valida√ß√µes robustas
- Tratamento de erros melhorado

---

## Checklist de Migra√ß√£o

### Prepara√ß√£o

- [ ] Fazer backup do c√≥digo atual
- [ ] Verificar Python 3.9+ instalado
- [ ] Ter Chrome instalado

### Instala√ß√£o

- [ ] Criar/ativar ambiente virtual
- [ ] Instalar depend√™ncias: `pip install -r requirements.txt`
- [ ] Criar arquivo `.env` com `OPENAI_API_KEY`
- [ ] Verificar `config.ini` presente

### Valida√ß√£o

- [ ] Testar execu√ß√£o: `python src/main.py`
- [ ] Verificar logs em `logs/application.log`
- [ ] Confirmar CSV gerado corretamente
- [ ] Validar resumos no JSON
- [ ] Testar envio para n8n (se aplic√°vel)

### Limpeza

- [ ] Remover c√≥digo antigo (opcional)
- [ ] Documentar mudan√ßas espec√≠ficas do projeto
- [ ] Atualizar documenta√ß√£o interna

---

## Solu√ß√£o de Problemas

### Erro: "OPENAI_API_KEY n√£o configurada"

**Solu√ß√£o**: Criar arquivo `.env` com a chave

```env
OPENAI_API_KEY=sk-...
```

### Erro: "Arquivo config.ini n√£o encontrado"

**Solu√ß√£o**: Copiar `config.ini` para raiz do projeto

### Erro: "ModuleNotFoundError: No module named 'src'"

**Solu√ß√£o**: Executar do diret√≥rio raiz do projeto

```bash
# Certo
cd d:\Developement\afirmanet\IA-AFN\Linkedin
python src/main.py

# Errado
cd src
python main.py
```

### Posts n√£o sendo processados

**Solu√ß√£o**: Verificar se j√° foram processados

```python
from src.database import DatabaseManager

db = DatabaseManager()
stats = db.get_statistics()
print(stats)  # Ver quantos j√° foram processados
```

Para reprocessar, deletar banco: `database/resumos_processados.db`

### Logs n√£o aparecem

**Solu√ß√£o**: Verificar n√≠vel de log em `config.ini`

```ini
[logging]
log_level = DEBUG  # Para mais detalhes
```

---

## Rollback

Se necess√°rio reverter para vers√£o anterior:

```bash
# Restaurar arquivo original
Move-Item -Path "src\DataBricks.py.backup" -Destination "src\DataBricks.py" -Force

# Executar vers√£o antiga
python src/DataBricks.py
```

**Nota**: A vers√£o antiga n√£o usa os novos arquivos de configura√ß√£o.

---

## Melhorias Futuras Sugeridas

1. **Testes Automatizados**
   - Implementar suite de testes com pytest
   - Coverage m√≠nimo de 80%

2. **CI/CD**
   - Pipeline de integra√ß√£o cont√≠nua
   - Deploy automatizado

3. **Monitoramento**
   - Dashboard de m√©tricas
   - Alertas autom√°ticos

4. **Performance**
   - Processamento paralelo
   - Cache de resultados

---

## Suporte

Para quest√µes sobre migra√ß√£o:

1. Consulte `docs/REFATORACAO_2025-12-09.md`
2. Revise `docs/ARQUITETURA.md`
3. Leia c√≥digo fonte (bem documentado)
4. Verifique logs em `logs/application.log`

---

## Conclus√£o

A migra√ß√£o para v2.0 traz:

- ‚úÖ C√≥digo mais limpo e organizado
- ‚úÖ Melhor manutenibilidade
- ‚úÖ Logs profissionais
- ‚úÖ Configura√ß√µes flex√≠veis
- ‚úÖ Tratamento robusto de erros
- ‚úÖ Documenta√ß√£o completa

**Tempo estimado de migra√ß√£o**: 15-30 minutos

**Risco**: Baixo (funcionalidade preservada + backup dispon√≠vel)

**Benef√≠cio**: Alto (c√≥digo profissional e escal√°vel)

---

*Boa migra√ß√£o!*

